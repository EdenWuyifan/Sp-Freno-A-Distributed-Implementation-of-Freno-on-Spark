{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import RDD, SparkConf, SparkContext\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tree import Tree, TreeNode\n",
    "import threading\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKER = 8\n",
    "\n",
    "def scanDB(path, seperation):\n",
    "    db = []\n",
    "    f = open(path, 'r')\n",
    "    for line in f:\n",
    "        if line:\n",
    "            temp_list = line.rstrip().split(seperation)\n",
    "            temp_list = [int(i) for i in temp_list]\n",
    "            temp_list.sort()\n",
    "            temp_list = [str(i) for i in temp_list]\n",
    "            db.append(temp_list)\n",
    "    f.close()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFreno(transactions, minsup):\n",
    "    # for each worker: input (transactions line, minsup) and return minsup list\n",
    "    \n",
    "    tree = Tree(minsup)\n",
    "    for trx in transactions:\n",
    "        tree.insert(tree._root,trx)\n",
    "    return tree.__repr__()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distFreno(inFile, min_sup, sc, k):\n",
    "\n",
    "    # Phase 1: generate Frequent items; produce vertical dataset\n",
    "\n",
    "    # tranData is ans RDD, consider using zipwithUniqueId\n",
    "    # TODO: add support for diffsets\n",
    "    \n",
    "    transDataRaw = scanDB(inFile, \" \")\n",
    "    numTrans = len(transDataRaw)\n",
    "    #print(transDataRaw[:5])\n",
    "    \n",
    "    minsup = min_sup * numTrans\n",
    "    #print(\"minsup\", minsup)\n",
    "    \n",
    "    out_rdd = []\n",
    "    for trx in transDataRaw:\n",
    "        out_rdd.extend([trx[i:] for i in range(len(trx))])\n",
    "    #print(out_rdd[:5])\n",
    "    \n",
    "    transDataFile = sc.parallelize(out_rdd)\n",
    "    #print(transDataFile.count())\n",
    "    \n",
    "    transData = transDataFile.map(lambda v: (v[0], v))\n",
    "    #print(transData.keys().take(5))\n",
    "    transData = transData.map(lambda v: v[1])\n",
    "    \n",
    "    transData = transData.groupBy(lambda v: int(v[0])%k).map(lambda v : (v[0], list(v[1]))).collect()#.sortByKey()\n",
    "    print(transData[0][1][:5])\n",
    "    \n",
    "    \n",
    "    #print(\"transaction data num of keys:\", transData.count())\n",
    "\n",
    "    #transDataList = transData.collect()\n",
    "    #print(transDataList[0])\n",
    "    \n",
    "    # use the configuration as the number of partitions\n",
    "    print(\"number of partitions used: {}\".format(sc.defaultParallelism))\n",
    "    # itemTidsParts = itemTids.repartition(sc.defaultParallelism).glom()\n",
    "    # print(itemTidsParts.take(5))\n",
    "\n",
    "    #phase 3: EClaT from k-itemsets\n",
    "    #print(transData.lookup(0).count())\n",
    "    #freqItemsList = freqItemTidsList.collect()\n",
    "    #freqAtoms = freqItemTidsList.keys().map(lambda k: [k]).collect()\n",
    "    # print(\"frequent items\", freqAtoms)\n",
    "    #freqRange = sc.parallelize(range(0, len(freqItemsList) - 1))\n",
    "    freqRange = sc.parallelize(range(0, k-1))\n",
    "    freqItemsListToRun = freqRange.map(\\\n",
    "        lambda v: transData[v])\n",
    "\n",
    "    #print('freqItemsListToRun', freqItemsListToRun.take(1)[0])\n",
    "    \n",
    "    #res = freqItemsListToRun.flatMap(lambda t: runEclat([[t[0][0]], t[0][1]], t[1], 2)).collect()\n",
    "    res = freqItemsListToRun.map(lambda t: runFreno(t[1],minsup)).collect()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30'], ['16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30'], ['24', '25', '26', '27', '28', '29', '30'], ['32', '33'], ['40', '41', '42', '43', '44', '45', '46', '47']]\n",
      "number of partitions used: 8\n",
      "[\"['40']\", \"['49']\", '[]', '[]', '[]', '[]', '[]']\n"
     ]
    }
   ],
   "source": [
    "testFiles = [\"retail\"]\n",
    "support = [0.4]\n",
    "\n",
    "conf = SparkConf().setAppName(\"\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "schema = StructType([\n",
    "    StructField(\"algorithm\", StringType(), False),\n",
    "    StructField(\"datasets\", StringType(), False),\n",
    "    StructField(\"support\", FloatType(), False)\n",
    "])\n",
    "for i in range(1):\n",
    "    schema.add(\"test{}\".format(i+1), FloatType(), True)\n",
    "#experiments = []\n",
    "\n",
    "for f in testFiles:\n",
    "    for s in support:\n",
    "        res = distFreno(\"./databases/{}.txt\".format(f), s, sc, 8)\n",
    "        print(res)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
